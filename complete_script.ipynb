{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "file_path = \"vector_store.pkl\"\n",
    "file_name=\"paper.pdf\"\n",
    "vector_file_path = os.path.join(\"vectorStore\",str.replace(file_name, 'pdf','pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"smollm\"\n",
    "from langchain_ollama import OllamaLLM\n",
    "model=OllamaLLM(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of pages loaded from the document are: 9\n",
      "\n",
      "Content of the first page:-------------------- \n",
      "\n",
      "page_content='RRCSNet:Recurrent Residual Convolutional Neural  \n",
      "Network based cloud segmentation from satellite  \n",
      "images  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Abstract— A crucial role in remote sensing applications,  \n",
      "particularly in satellite imagery analysis, is played by cloud  \n",
      "segmentation.  In this paper,  RRCSNet,  a novel  Recurrent  \n",
      "Residual Convolutional Neural Network designed for accurate  \n",
      "and efficient  cloud  segmentation.  The architecture  combines  the \n",
      "strengths  of recurrent  neural  networks  and residual  \n",
      "convolutional  neural  networks  and enhance  feature  \n",
      "representation.  The model  learns  sequential  patterns  in satellite  \n",
      "data through recurrent connections, and residual connections  \n",
      "help train  deeper  networks  by addressing  the vanishing  gradient  \n",
      "problem.  Experimental  results  on Landsat  satellite  datasets  \n",
      "prove the higher performance of RRCSNet compared to the  \n",
      "existing  cloud  segmentation  methods.  Compared  to several  \n",
      "other networks, this method establishes superior performance  \n",
      "in cloud detection.  \n",
      " \n",
      "Keywords —cloud,  shadow,  recurrent  network,  residual  \n",
      "connection,  satellite image,  deep  learning  \n",
      "I. INTRODUCTION  \n",
      "Satellite  imagery  has become  an invaluable  resource  for a \n",
      "multitude  of applications,  ranging  from  environmental  \n",
      "monitoring  to disaster  response  and urban  planning.  However,  \n",
      "the accurate interpretation of these images is often impeded by  \n",
      "the presence  of clouds,  which  can obscure  critical  information  \n",
      "and hinder  subsequent  analyses.  Cloud  segmentation  in \n",
      "satellite  images  is, therefore,  a crucial  task in remote  sensing,  \n",
      "prompting the reliability and effectiveness of downstream  \n",
      "applications.  \n",
      "Clouds and cloud shadows can hide ground information  \n",
      "and significantly limiting the quality of scene understanding.  \n",
      "In cloud  covered  images  it is difficult  to obtain  ground  \n",
      "information.  Cloud  contaminated  regions  in the remote  \n",
      "sensing images appear high intensity regions and shadow  \n",
      "covered regions appear as dark. Clouds have varying shape,  \n",
      "they move  and change  their appearance.  \n",
      "Traditional  cloud  segmentation  methods  have  met \n",
      "challenges in handling the complex and dynamic nature of  \n",
      "cloud  cover,  particularly  in the presence  of diverse  \n",
      "atmospheric  conditions.  Traditional  methods  for cloud  \n",
      "segmentation  often  struggle  with issues  such as low accuracy,  \n",
      "especially in the presence of complex cloud structures and  \n",
      "varying  atmospheric  conditions.  This necessitates  an advanced  approach  that can effectively  learn  and retain  both \n",
      "local  and global  features  from  satellite  images.  \n",
      " \n",
      " \n",
      "Fig. 1 Input  cloudy  satellite  image  \n",
      " \n",
      "Fig. 2 Ground  truth  image  \n",
      "To address this gap, RRCSNet is proposed  , a Recurrent  \n",
      "Residual  Convolutional  Neural  Network  tailored  for \n",
      "segmenting  clouds  in satellite  images.  RRCSNet  \n",
      "amalgamates the strengths of recurrent neural networks and  \n",
      "residual  convolutional  neural  networks  to effectively  enhance  \n",
      "feature  representation.  The residual  connections  facilitate  the \n",
      "training of deep networks by mitigating issues such as the  \n",
      "vanishing  gradient  problem.  \n",
      "This paper presents the architecture, design principles, of  \n",
      "RRCSNet , showcasing its superior performance compared to  \n",
      "existing cloud segmentation methodologies. The proposed  \n",
      "RRCSNet  marks  a major  breakthrough  in cloud  segmentation  \n",
      "from satellite images, providing a dependable and scalable  \n",
      "solution  with broad  applications  in earth  observation  systems.  \n",
      "II. RELATED  WORKS  \n",
      "K. K. Jena et al. [1] proposed  a Deep  Convolutional  Neural  \n",
      "Network model with many layers which extracts features  \n",
      "through  convolution  and down  sampling  operation  to classify  \n",
      "cloudy  satellite  images.  Attention  based  U-Net architecture,  is \n",
      "used to detect  clouds  by methods  proposed  by S. Singhal  et al. \n",
      "[2]. A cloud  segmentation  network  by fusing  multiscale' metadata={'source': 'paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(file_name)\n",
    "pages=loader.load_and_split()\n",
    "# print(f\"the number of pages loaded from the document are: {len(pages)}\\n\\nContent of the first page:{'-'*20} \\n\\n{pages[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer the question asked on the basis of the context provided.\n",
      "If you can not find the answer then just say \"answer is not available in the given text\n",
      "context : Here is the context\n",
      "Question : Here is the question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\" Answer the question asked on the basis of the context provided.\n",
    "If you can not find the answer then just say \"answer is not available in the given text\n",
    "context : {context}\n",
    "Question : {question}\n",
    "\"\"\"\n",
    "prompt= PromptTemplate.from_template(template=template)\n",
    "# print(prompt.format(context=\"Here is the context\", question=\"Here is the question\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings=OllamaEmbeddings(model=\"smollm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "vector_store=DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'paper.pdf', 'page': 2}, page_content='dimensionality  of features.  The activation  function  ReLU  is \\ndefined  as \\nReLU(X)=max(0,X)  \\nTo incorporate low -level details of cloud with high -level  \\nfeatures from feature maps in different scales full scale skip  \\nconnections are added in the encoder path.  The input image  \\nundergoes multiple layers of convolutions and pooling in the  \\nencoding  path,  with recurrent  residual  blocks  enhancing  \\nfeature  extraction  at each stage.  These  features  are then \\nprocessed  through  a series of  residual  blocks,  enhancing  \\nfeature learning and retention. The output from the residual  \\nblocks is fed into recurrent layers, which capture long -range  \\ndependencies  and refine  the segmentation  mask.  The recurrent  \\nconnections are built using GRU units that process the input  \\nfeature maps recurrently. The GRU gates help in selectively  \\nremembering and forgetting information, thus enabling the  \\nnetwork  to learn temporal  dependencies.  \\nTable 1  Structure  of the  network  of convolution. The up -sampling help to confine the cloud  \\npixels,  in concatenation,  the output  of the corresponding  \\nencoder block is concatenated with up sampled values and  \\nfrom the scaled down versions from the previous layers. The  \\nresult  of concatenation  is given  recurrent  residual  \\nconvolutional neural network block same as done in encoder  \\npart. The number of filters used in first decoder block is 48,  \\nand in each decoder  block  the number  of filters  is reduced  by \\n2. Finally  in the last stage  1x1 convolution  is performed  with \\nsigmoid activation function which will produce an output  \\nimage  with same  size as input  image.  Then  binary  cloud  mask  \\nis generated.  \\nsigmoid(x)=1/(1+e( -x)) \\nThe decoding  path then upsamples  the feature  maps,  with \\nskip connections from the encoding path ensuring that high - \\nresolution  features  are preserved.  The final output  is a \\nsegmentation  map,  where  each pixel  is classified  according  to \\nthe learned features. Table 1 describes the structure of the  \\nnetwork.  \\n \\n \\nIV. RESULTS  AND  DISCUSSION  \\nThe data set that is used in this paper  is obtained  from  [14] \\nwhich  consists  of Landsat  8 images  and corresponding  ground  \\ntruth.  It contains  80, 1000  x 1000 -pixel  cloud  validation  data.  \\nIt extracted  several  slightly  overlapping  and non over lapping  \\n256x256  patches  from  these  images  and generated.  The \\nproposed  RRCSNet  is implemented  with the Keras  \\nframework.  \\nEvaluation  Metrics  \\nAfter creating the cloud and shadow mask of the test set,  \\nthe predicted  cloud  or shadow  mask  is compared  to the \\noriginal  cloud  or shadow  mask  to find out the similarity.  \\nThe performance of the proposed RRCSNet  is measured  \\nby precision,  Jacacard  Index,  dice coefficient  and recall.  \\nAll the metrics  are denoted  as following:  \\n𝑇𝑃 \\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛  =  \\n \\n(𝑇𝑃 + 𝐹𝑃) \\n𝑅𝑒𝑐𝑎𝑙𝑙  = 𝑇𝑃\\n \\n(𝑇𝑃+𝐹𝑁) \\nDice coefficient [15] is a similarity coefficient that is used to  \\ncompare the similarity of actual cloud Y and predicted cloud  \\nY^.The  result  is a number  between  0 and 1, where  1 indicates  \\nthat the actual  and predicted  masks  are identical.  \\n𝐷𝑖𝑐𝑒 (𝑌, 𝑌^) = 2 ∗ 𝑇𝑃 \\n(2 ∗ 𝑇𝑃 + 𝐹𝑃 + 𝐹𝑁) \\n𝑌 ∩ 𝑌^ \\n𝐽𝑎𝑐𝑐𝑎𝑟𝑑  𝐼𝑛𝑑𝑒𝑥 (𝑌, 𝑌^) =  \\n \\n𝑌 𝖴 𝑌^ \\n3.2 Decoder  \\nThe decoder blocks up samples the cloud and shadow  \\nfeatures  back  to the original  image  size. To expand  the size of \\nimages each decoder block uses an up -sampling technique  \\ncalled transposed convolution. The decoder portion of the  \\nnetwork  entails  of four decoder  blocks.  The convolution  \\noperation with kernel size (3,3) and stride is 2 performed by  \\ndecoder block is transposed convolution. It is used to rescale  \\nthe feature  map.  It is followed  by concatenation  and two layers  Where FP represent actual non -cloud pixels in the test image  \\nis predicted  as cloud,  false  negative  is a cloud  pixel  is \\npredicted as non -cloud, true negative is the non -cloud pixels'),\n",
       " Document(metadata={'source': 'paper.pdf', 'page': 0}, page_content='used to detect  clouds  by methods  proposed  by S. Singhal  et al. \\n[2]. A cloud  segmentation  network  by fusing  multiscale  \\nspatial  and spectral features proposed by J. Li et al. [3]. A  \\nsaliency  based  mixed  feature  map based  method  is utilized  to \\ndetect  clouds  in satellite  images  is proposed  by M. Wang  et al.'),\n",
       " Document(metadata={'source': 'paper.pdf', 'page': 1}, page_content=\"[4]. This method  used mean  value  of bands  and haze \\noptimized  transformation  to find out the saliency  \\ncharacteristics. In this method Ostu’s thresholding is applied  \\nto find out the  initial clod mask then it is refined by guided  \\nfiltering.  Two cascaded  CNN  model  are used to detect  clouds  \\nby method proposed by M. Luotamo et al.[5]. To extract  \\nrobust  features  and reduce  the number  of parameters  a \\nlightweight vision transformer network is used to detect the  \\nclouds by B. Zhang et al.[6]. A multi scale dark channel  \\nextractor  is used to predict  dark channels  and to extract  robust  \\nfeatures  for accurate  cloud  segmen tation,  dark channel  \\nfeatures  and image  features  are input  to the aggregation  \\nmodule. S. Mohajerani et al. [7], proposed filtered Jaccard loss  \\nis to calculate  the error  while  segmenting  cloud  pixels.  S. Zhu \\net al.[8] proposed a weakly supervised learning method for  \\ncloud  detection  in multisensor  images.  To address  the problem  \\nof long training  times  required  by current  deep  learning  \\nmodels,  J. Qian  et al. [9] developed  an improved  cloud  \\nDeepLabV3+ network. A U -Net++ semantic segmentation  \\nmodel was proposed to detect clouds in satellite image by  \\nPreetpal Kaur Buttar et al. [10].  An attention -aided multilevel  \\ncloud detection network proposed for detecting clouds in  \\noptical satellite image by H. Zhai et al. [11].  To enhance the  \\naccuracy of cloud detection in high -brightness scenes, Y.  \\nChen  et al. [12] proposed  a cloud  index -driven  spatial -spectral  \\ncontext  attention  network  for cloud  detection.  A cloud  \\ndetection  algorithm  based  on land type has been  proposed  by \\nX. Zhou  et al. [13]. Threshold  values  are selected  by utilizing  \\nsupport  information  from  different  datasets.  \\nEven though above -mentioned methods yield good results,  \\nbut difficult for varying landscape and in the boundary of  \\nclouds. By combining information from red, blue, and near - \\ninfrared  bands,  particularly  the differences  in reflectance  \\nbetween  clouds  and non-cloud  surfaces,  remote  sensing  \\nalgorithms can effectively detect and characterize clouds in  \\nsatellite imagery. This information is valuable for various  \\napplications,  including  weather  forecasting,  climate  \\nmonitoring,  and environmental  research.  \\nIII. PROPOSED  METHOD  \\nThe proposed method utilizes near -infrared band, red and  \\nblue bands  of Landsat  8 which  are commonly  used in remote  \\nsensing for cloud detection and characterization. Each of these  \\nbands  provides  unique  spectral  information  that aids in \\ndistinguishing  clouds  from  other  features  on the earth's  \\nsurface.  In this Recurrent  Residual  Convolutional  Neural  \\nNetwork  based  cloud  segmentation  from  satellite  image  \\nRRCSNet, the input multi spectral satellite image undergoes  \\nseries of convolution, batch normalization, activation down  \\nsampling  and up sampling.  The architecture  of proposed  \\nmethod is shown in Fig.3. The structure of encoder block is  \\nshown  in Fig 4. Due to efficiency  and simplicity  gated  \\nrecurrent  unit is used in RRCSNet.  Encoder  and decoder  part \\nsupport residual and recurrent connection. The goal of the  \\nproposed method is to develop a neural network capable of  \\naccurately segmenting each pixel in future images based on  \\nproperties  learned  from  ground  truth  data.  \\nThe proposed  Recurrent  Residual  Convolutional  Neural  \\nNetwork  based  cloud  segmentation  from  satellite  image  \\nRRCSNet,  consists  of training  and testing  steps.  In the training  \\nphase,  cloud contaminated images with its labelled images  \\nand validation  images  are given  as input.  After  training  testing  \\nstep is done  to segment  the clouds  in the test set. The network  \\nis based  on an encoding -decoding  architecture.  Finally,  the network  outputs  a matrix  containing  the classification  of each \\npixel.  \\n3.1 Encoder\"),\n",
       " Document(metadata={'source': 'paper.pdf', 'page': 3}, page_content='a \\n \\n \\n \\nb \\n \\n \\n \\nc \\n \\n \\n \\nd \\n \\n \\nFig. 6 Cloud  detection  results  of sample  images  from  the \\ndataset.  a) Original  cloudy  satellite  image  b) ground  truth  \\nc) cloud  detected  by proposed  approach  d) cloud  detected  \\nby VGG16  \\n \\na \\n \\n \\n \\n \\nb \\n \\n \\n \\nc \\n \\n \\n \\nd \\n \\n \\nFig. 7 Cloud  detection  results  of sample  images  from  the \\ndataset.  a) Original  cloudy  satellite  image  b) ground  truth  \\nc) cloud  detected  by proposed  approach  d) cloud  detected  \\nby VGG16  \\nTable 2 Comparison of RRCSNet  evaluated on the  \\ndataset  [14] for cloud  detection  with different  encoder  \\nbackbones.  RRCSNet  is measured  through  number  of different  \\nexperiments with  different  landscape,  with varying  cloud  \\npercentage,  with thin and thick  cloud.  The result  of the \\nRRCSNet  with sample images from the dataset is shown in  \\nFig.6 and Fig. 7. From Fig.6 and Fig.7 it is clear that the  \\nproposed Recurrent Residual Convolutional Neural Network  \\nRRCSNet  has got promising  results.  In Fig.6d  the red \\nrectangles  represents  the misclassification  of pixels  compared  \\nto pixels in Fig 6.c and Fig 6 b . In Fig. 6 d first and third  \\ncolumn  in the red rectangle  no pixels  detected  as cloud  \\ncompared to ground truth. In Fig. 7 in case of images with  \\nlarge percentage of cloud cover also got results closer to  \\nground truth. The training time for RRCSnet is generally  \\nlonger  compared  to simpler  models.  Training  recurrent  layers  \\nis computationally  more  intensive  than training  standard  \\nconvolutional  layers  . \\nThe GRU  recurrent  layers  in RRCSnet  add a significant  \\nnumber  of parameters  compared  to standard  CNNs,  increasing  \\nthe overall  model  size. Additionally,  residual  connections  add \\nto the complexity  and memory  requirements.  However,  these  \\nadded parameters are crucial for capturing temporal dynamics,  \\nwhich can lead to better performance on tasks like cloud  \\ndetection. Training a model like RRCSnet involves careful  \\ntuning  of various  hyperparameters  to achieve  optimal  \\nperformance.  \\nTable  3 Hyperparameter  settings  \\n \\nName  Parameter  Value  \\nLearning  rate 1e-4 \\nOptimizer  Nadam  \\nLoss  function  Dice  loss \\nWeight  Initialization  He normal  \\nBatch  Size 8 \\nNumber  of Epochs  1700  \\n \\n \\n \\na b c \\nFig. 8 Cloud detection results of sample images from the  \\ndataset.  a) Original  cloudy  RGB  satellite  image  b) ground  \\ntruth  c) cloud  detected  by proposed  approach  \\nModel  Dice  Recall  Precision  Jaccard  \\nIndex  \\nRRCSNet  0.9055  0.9276  0.9167  0.9506  \\nFMask  - 0.914  0.7837  0.7301  \\nInception  \\nv3 0.8803  0.9220  0.9381  0.9331  \\nSegNet  - 0.8933  0.8335  - \\nVGG16  0.8984  0.8922   0.9480  \\nDeeplabV  \\n3+ 0.8688  0.8976  0.8958  -')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vector_store.as_retriever()\n",
    "# retriever.invoke(\"What is the result of the study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = ({\n",
    "    \"context\":itemgetter(\"question\") | retriever,\n",
    "    \"question\":itemgetter(\"question\")\n",
    "    }|\n",
    "    prompt|model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: atmospheric  conditions.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\":\"What is the result\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the title of the paper\n",
      "Answer:The title of the paper is \"Deep Convolutional Neural Networks for Multiscale Cloud Detection in Satellite Imagery\"\n",
      "\n",
      "question: How many authors are there in the paper\n",
      "Answer:The paper \"A Recurrent Residual Convolutional Neural Network for Cloud Segmentation from Satellite Imagery\" by Xue et al. (2020) has 5 authors listed on its author page.\n",
      "\n",
      "To find all authors of a paper, you can use the following steps:\n",
      "\n",
      "1. Go to the paper's author page and look for the \"Authors\" section.\n",
      "2. Check if there are multiple authors listed. If so, separate them by commas (e.g., \"Xue, Y., Zhang, H., Li, J., & Chen, X.\").\n",
      "3. If there is only one author listed, you can stop here and use the paper's title or abstract to find more information about the research.\n",
      "\n",
      "question: what is the plateform used to run the experiment\n",
      "Answer:The plateform used to run the experiment in this paper is likely a high-performance computing (HPC) cluster or a distributed computing environment. The specific details of the plateform are not provided, but it's clear that the researchers were using a powerful and scalable system to support their experiments.\n",
      "\n",
      "question: summerize the methodology\n",
      "Answer:The proposed method is a neural network-based approach for cloud segmentation from satellite images using multi-spectral bands. The architecture consists of an encoder block, which extracts features from the input image, and a decoder block, which generates the final output. The training process involves training the network on labelled images and testing it on validation images to improve its performance.\n",
      "\n",
      "The proposed method is based on the idea that multi-spectral bands can provide more information about cloud properties than single-spectral bands. By using multiple spectral bands, the network can learn to extract features that are specific to clouds or non-cloud surfaces, and use these features to segment clouds from other surfaces in the image.\n",
      "\n",
      "The proposed method has several advantages over traditional computer vision techniques for cloud segmentation:\n",
      "\n",
      "1. **Improved accuracy**: The proposed method is based on a neural network architecture that can learn to extract features from images that are specific to clouds or non-cloud surfaces, which can lead to improved accuracy compared to traditional methods.\n",
      "2. **Robustness to variations in image quality**: The proposed method can handle variations in image quality, such as changes in resolution or lighting conditions, which can affect the performance of traditional computer vision techniques.\n",
      "3. **Flexibility and adaptability**: The proposed method can be easily adapted to different satellite images and cloud types by fine-tuning the network on a specific dataset.\n",
      "4. **Ability to handle clouds with varying properties**: The proposed method can handle clouds with varying properties, such as different shapes, sizes, or textures, which is not possible with traditional computer vision techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# questions=[\n",
    "#     \"What is the title of the paper\", \n",
    "#     \"How many authors are there in the paper\",\n",
    "#     \"what is the plateform used to run the experiment\", \n",
    "#     \"summerize the methodology\"\n",
    "# ]\n",
    "\n",
    "# for question in questions:\n",
    "#     print(f\"question: {question}\")\n",
    "#     print(f\"Answer:{chain.invoke({'question':question})}\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
